{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-08 14:08:41.005343: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-08 14:08:41.015764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-08 14:08:41.030640: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-08 14:08:41.034251: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-08 14:08:41.047327: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-08 14:08:41.767924: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from cupy import ndarray\n",
    "from typing import List, Any, Tuple, Set\n",
    "\n",
    "import cupy\n",
    "import nodeleys as ndl\n",
    "import sys\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "\n",
    "def t_update(ndl):\n",
    "  pending_for_deletion = []\n",
    "  for i in sys.modules:\n",
    "    if i.split('.')[0] == 'nodeleys':\n",
    "      pending_for_deletion.append(i)\n",
    "\n",
    "  # print(pending_for_deletion)\n",
    "    \n",
    "  for i in pending_for_deletion:\n",
    "    del sys.modules[i]\n",
    "\n",
    "  import nodeleys as ndl\n",
    "  return ndl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139675480337360\n",
      "139675480341392\n",
      "--- 0.006412029266357422 seconds ---\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[  -70.30361795   184.90510416   174.601976   ...    18.37730079\n",
      "    -48.71095836   141.26876165]\n",
      " [-2603.35753478  -448.69451005   837.66135326 ...   607.91020821\n",
      "    767.20913      323.37527932]\n",
      " [  153.3160691    145.82640578   -64.35829077 ...  -202.83654486\n",
      "     53.59597474    57.58094397]\n",
      " ...\n",
      " [ -653.83530846  1684.32158775  4655.5048778  ...   599.55382566\n",
      "   -269.77455727   342.54764375]\n",
      " [  377.04126812   200.37490756    27.33831238 ...  -128.32926958\n",
      "     29.54842173  -260.11906441]\n",
      " [ -274.78516072  1225.40299049  -525.70792783 ...   297.41351683\n",
      "   -743.41198548  -223.9332323 ]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[ 1.02737875e+00  1.28907020e+00 -4.14769948e-01 ...  3.89668790e-01\n",
      "  -1.55186727e+00  3.32972538e-01]\n",
      " [-3.91035451e+01 -2.38819928e+00  4.88412008e+00 ...  2.92026025e+00\n",
      "  -1.50652611e+01  6.11152385e+00]\n",
      " [-5.49419992e-02  4.24107784e-02  2.11556717e-02 ...  7.31638207e-02\n",
      "  -9.03192001e-02 -9.47004668e-03]\n",
      " ...\n",
      " [ 3.19542570e-02  5.88939352e-02  5.35220185e-03 ...  1.38097027e-02\n",
      "   8.25915746e-03  6.42825954e-02]\n",
      " [ 2.03868129e-01 -2.38515697e-01 -2.50664167e-04 ...  2.44878037e-01\n",
      "   4.57091361e-02  6.65203128e-01]\n",
      " [-3.58505578e+02  1.85317177e+01 -1.20191968e+02 ... -9.12250834e+01\n",
      "  -8.27458047e+02  4.10915558e+02]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[ 1.96974386e+00  1.47101072e+00 -1.04985303e+00 ... -1.90226179e-01\n",
      "  -1.44355038e+00  5.70021437e-01]\n",
      " [-1.50714440e+01 -4.13961674e+00 -1.30683831e+00 ... -1.41001869e+00\n",
      "  -9.35376559e+00 -2.34493861e+00]\n",
      " [-2.96337372e-02 -2.65196719e-02  1.71214692e-02 ...  1.82655593e-01\n",
      "  -3.67014010e-02 -1.73088695e-02]\n",
      " ...\n",
      " [-3.60241482e-02  7.85396480e-02  2.39320371e-03 ... -5.77383181e-02\n",
      "   1.34758824e-02 -2.73005726e-02]\n",
      " [ 3.52917021e-02  1.47198896e-01  1.89819862e-02 ... -1.07269664e-01\n",
      "   1.55741499e-02  7.27414212e-01]\n",
      " [ 4.33287833e+01  1.88583956e+02  1.04733804e+02 ... -4.29644030e+01\n",
      "   1.28629041e+02 -6.92064517e+02]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "--- 0.21475815773010254 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "ndl = t_update(ndl)\n",
    "from nodeleys.math.forward_math_func import *\n",
    "from nodeleys.graph import Switch\n",
    "\n",
    "data_x = ndl.Node(cupy.random.normal(size=(1516,1516)), name='x')\n",
    "data_y = ndl.Node(cupy.random.normal(size=(1516,1)), name='y')\n",
    "data_y.is_constant = True\n",
    "\n",
    "w0 = ndl.Node(cupy.random.normal(size=(1516,516)), name='w0')\n",
    "b0 = ndl.Node(cupy.random.normal(size=(1,516)), name='b0')\n",
    "\n",
    "w1 = ndl.Node(cupy.random.normal(size=(516,64)), name='w1')\n",
    "b1 = ndl.Node(cupy.random.normal(size=(1,64)), name='b1')\n",
    "\n",
    "w1r = ndl.Node(cupy.random.normal(size=(516,64)), name='w1r')\n",
    "b1r = ndl.Node(cupy.random.normal(size=(1,64)), name='b1r')\n",
    "\n",
    "rR = ndl.Node(cupy.random.normal(size=(64,64)), name='rR')\n",
    "bR = ndl.Node(cupy.random.normal(size=(1,64)), name='bR')\n",
    "\n",
    "rL = ndl.Node(cupy.random.normal(size=(64,64)), name='rL')\n",
    "bL = ndl.Node(cupy.random.normal(size=(1,64)), name='bL')\n",
    "\n",
    "w2 = ndl.Node(cupy.random.normal(size=(64,1)), name='w2')\n",
    "b2 = ndl.Node(cupy.random.normal(size=(1,1)), name='b2')\n",
    "\n",
    "RANX = ndl.Node(cupy.random.normal(size=(1516, 64)), name='RANX')\n",
    "\n",
    "gamma0 = node_matmul(data_x, w0, 'gamma0')\n",
    "y0 = node_add(gamma0, b0, 'y0')\n",
    "\n",
    "gamma1 = node_matmul(y0, w1, 'gamma1')\n",
    "y1 = node_add(gamma1, b1, 'y1')\n",
    "\n",
    "gamma1r = node_matmul(y0, w1r, 'gamma1r')\n",
    "y1rs = node_add(gamma1r, b1r, 'y1rs')\n",
    "\n",
    "y1r = Switch(y1rs, [gamma1r], [node_mul(y1rs, 0.), node_mul(RANX, y1rs), node_mul(y1rs, 0.)], ['x<=0', 'x>0', 'x<=0'], 'y1r').compile()\n",
    "# y1r = Virtual([y1r_pre], [node_mul(y1r_pre, 0.), y1r_pre], ['[0]<0', '[0]>=0'], 'y1r').compile()\n",
    "\n",
    "gamma_rR = node_matmul(y1r, rR, 'gamma_rR')\n",
    "y_rR = node_add(gamma_rR, bR, 'y_rR')\n",
    "\n",
    "gamma_rL = node_matmul(y1r, rL, 'gamma_rL')\n",
    "y_rL = node_add(gamma_rL, bL, 'y_rL')\n",
    "\n",
    "y_rRL = node_add(y_rR, y_rL, 'y_rRL')\n",
    "y_rRL = node_div(5., node_add(5., y_rRL))\n",
    "\n",
    "merge = node_add(y1r, y_rRL, 'merge')\n",
    "merge2 = node_add(merge, y1, 'merge2')\n",
    "\n",
    "gamma2 = node_matmul(merge2, w2, 'gamma2')\n",
    "y2 = node_add(gamma2, b2, 'y2')\n",
    "\n",
    "diff = node_sub(y2, data_y, 'diff')\n",
    "squared = node_pow(diff, 2., 'squared')\n",
    "redsum = node_redsum(squared, 0, 'redsum')\n",
    "loss = node_div(redsum, 16., 'loss')\n",
    "\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "  tf_data_x = tf.Variable(tf.convert_to_tensor(data_x.tensor.get()))\n",
    "  tf_data_y = tf.Variable(tf.convert_to_tensor(data_y.tensor.get()))\n",
    "  tf_w0 = tf.Variable(tf.convert_to_tensor(w0.tensor.get()))\n",
    "  tf_b0 = tf.Variable(tf.convert_to_tensor(b0.tensor.get()))\n",
    "  tf_w1 = tf.Variable(tf.convert_to_tensor(w1.tensor.get()))\n",
    "  tf_b1 = tf.Variable(tf.convert_to_tensor(b1.tensor.get()))\n",
    "  tf_w1r = tf.Variable(tf.convert_to_tensor(w1r.tensor.get()))\n",
    "  tf_b1r = tf.Variable(tf.convert_to_tensor(b1r.tensor.get()))\n",
    "  tf_rR = tf.Variable(tf.convert_to_tensor(rR.tensor.get()))\n",
    "  tf_bR = tf.Variable(tf.convert_to_tensor(bR.tensor.get()))\n",
    "  tf_rL = tf.Variable(tf.convert_to_tensor(rL.tensor.get()))\n",
    "  tf_bL = tf.Variable(tf.convert_to_tensor(bL.tensor.get()))\n",
    "  tf_w2 = tf.Variable(tf.convert_to_tensor(w2.tensor.get()))\n",
    "  tf_b2 = tf.Variable(tf.convert_to_tensor(b2.tensor.get()))\n",
    "  tf_RANX = tf.Variable(tf.convert_to_tensor(RANX.tensor.get()))\n",
    "\n",
    "  tf_gamma0 = tf_data_x @ tf_w0\n",
    "  tf_y0 = tf_gamma0 + tf_b0\n",
    "  tf_gamma1 = tf_y0 @ tf_w1\n",
    "  tf_y1 = tf_gamma1 + tf_b1\n",
    "  tf_gamma1r = tf_y0 @ tf_w1r\n",
    "  tf_gamma1rs = tf_y0 @ tf_w1r\n",
    "  print(id(tf_gamma1r))\n",
    "  print(id(tf_gamma1rs))\n",
    "  tf_y1rs = tf_gamma1r + tf_b1r\n",
    "  tf_y1r = tf.where(tf_y1rs <= 0, x=0., y=tf_RANX * tf_y1rs)\n",
    "\n",
    "\n",
    "  tf_gamma_rR = tf_y1r @ tf_rR\n",
    "  tf_y_rR = tf_gamma_rR + tf_bR\n",
    "\n",
    "  tf_gamma_rL = tf_y1r @ tf_rL\n",
    "  tf_y_rL = tf_gamma_rL + tf_bL\n",
    "\n",
    "  tf_y_rRL = tf_y_rR + tf_y_rL\n",
    "  tf_y_rRL = 5./(5. + tf_y_rRL)\n",
    "\n",
    "  tf_merge = tf_y1r + tf_y_rRL\n",
    "  tf_merge2 = tf_merge + tf_y1\n",
    "\n",
    "  tf_gamma2 = tf_merge2 @ tf_w2\n",
    "  tf_y2 = tf_gamma2 + tf_b2\n",
    "  tf_diff = tf_y2 - tf_data_y\n",
    "  tf_squared = tf_diff ** 2\n",
    "  tf_redsum = tf.reduce_sum(tf_squared, axis=0, keepdims=True)\n",
    "  tf_loss = tf_redsum / 16.\n",
    "  \n",
    "start_time = time.time()\n",
    "grads = g.gradient(tf_loss, [tf_w0, tf_w1, tf_w2, tf_b0, tf_gamma_rR])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "loss.adic.set_as_objective()\n",
    "tr = []\n",
    "loss.adic.begin_backprop(tracing=False, traces=tr)\n",
    "w0.get_gradient()\n",
    "w1.get_gradient()\n",
    "w2.get_gradient()\n",
    "b0.get_gradient()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2216719.5704072 ,   626394.19438614,  1036660.34502255, ...,\n",
       "         -702777.92528456,   504742.17586609,   706562.05247141],\n",
       "       [-2811134.53332952,  -723578.90306667,  1548738.58727583, ...,\n",
       "         -833941.6211343 ,   761030.65744399,  -473151.31407247],\n",
       "       [ 1776133.20816927, -1271125.69306865, -1844830.63223404, ...,\n",
       "          943392.38596944,    99485.02254197, -1124991.95308106],\n",
       "       ...,\n",
       "       [ 2913166.71718057, -2126224.01789967, -3290952.24563971, ...,\n",
       "         1861586.61439851,  -581835.17322673, -1341909.18287818],\n",
       "       [-1446029.15781861,  -905766.94022788,  1232509.50423724, ...,\n",
       "          -34431.38682223,   223714.09886089,    76916.19403916],\n",
       "       [ -118842.22707994,  1001365.32026086,   220777.97495195, ...,\n",
       "         -354913.68735439,  -455888.06209672,   841034.92337224]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0.get_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1516, 516), dtype=float64, numpy=\n",
       "array([[-2216719.57040721,   626394.19438614,  1036660.34502255, ...,\n",
       "         -702777.92528457,   504742.17586609,   706562.05247141],\n",
       "       [-2811134.53332953,  -723578.90306667,  1548738.58727583, ...,\n",
       "         -833941.6211343 ,   761030.657444  ,  -473151.31407247],\n",
       "       [ 1776133.20816927, -1271125.69306865, -1844830.63223405, ...,\n",
       "          943392.38596944,    99485.02254197, -1124991.95308107],\n",
       "       ...,\n",
       "       [ 2913166.71718056, -2126224.01789968, -3290952.2456397 , ...,\n",
       "         1861586.61439851,  -581835.17322673, -1341909.18287819],\n",
       "       [-1446029.1578186 ,  -905766.94022788,  1232509.50423725, ...,\n",
       "          -34431.38682223,   223714.09886089,    76916.19403916],\n",
       "       [ -118842.22707994,  1001365.32026086,   220777.97495195, ...,\n",
       "         -354913.68735439,  -455888.06209672,   841034.92337224]])>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04710433  0.54420619  1.17574159  0.21813486  1.68257626  0.3986281\n",
      "  -0.34513129 -0.58590319  0.29687417  1.85978098]\n",
      " [-1.28544328 -1.28013757 -0.01488046  0.05005914 -2.13714622  0.64959816\n",
      "  -0.90078925 -1.66003304 -0.50783626 -0.17317154]\n",
      " [-0.63254889  0.43727574  0.17025356  0.87201782  0.36658144  1.82580419\n",
      "  -1.26871636 -0.51437269  0.52708737  0.86097004]\n",
      " [-0.46030965 -0.62733715 -1.62119988 -1.27459283  1.01374223  1.08245333\n",
      "   0.4510399   0.50277866 -0.75068139  0.22130257]\n",
      " [ 0.53317996  0.26758735 -1.95538583 -0.49527181 -0.04385985 -0.17831207\n",
      "  -2.37670143 -1.80776075 -1.16241916  0.33524723]\n",
      " [-0.57546061  1.10798424 -2.06029833 -0.56538354  0.05843647  0.07421798\n",
      "   2.23315375  1.61805131  0.06551809 -0.70496261]\n",
      " [ 1.06541693 -0.28347132  0.28232949  0.79915137  1.2663597   0.5415899\n",
      "   0.87491735  1.6707235  -0.29585109 -0.90180896]\n",
      " [ 1.15023611 -0.97510714 -0.23937976 -0.00483921  1.0582596   0.37728426\n",
      "  -0.15567885 -0.1979183   0.49094529  1.40132904]\n",
      " [ 1.20165866 -1.10148236 -0.56408872 -0.33622698  0.36867938  0.02011754\n",
      "  -1.18101047  2.04187731  0.35059333  0.15891649]\n",
      " [ 0.87188742 -0.44888228  0.01976177 -0.68164666  1.62011792  0.6963296\n",
      "   0.04203325 -0.59396937  1.20494663 -0.17879334]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.54420619, 1.17574159, 0.21813486, 1.68257626,\n",
       "        0.3986281 , 0.        , 0.        , 0.29687417, 1.85978098],\n",
       "       [0.        , 0.        , 0.        , 0.05005914, 0.        ,\n",
       "        0.64959816, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.43727574, 0.17025356, 0.87201782, 0.36658144,\n",
       "        1.82580419, 0.        , 0.        , 0.52708737, 0.86097004],\n",
       "       [0.        , 0.        , 0.        , 0.        , 1.01374223,\n",
       "        1.08245333, 0.4510399 , 0.50277866, 0.        , 0.22130257],\n",
       "       [0.53317996, 0.26758735, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.33524723],\n",
       "       [0.        , 1.10798424, 0.        , 0.        , 0.05843647,\n",
       "        0.07421798, 2.23315375, 1.61805131, 0.06551809, 0.        ],\n",
       "       [1.06541693, 0.        , 0.28232949, 0.79915137, 1.2663597 ,\n",
       "        0.5415899 , 0.87491735, 1.6707235 , 0.        , 0.        ],\n",
       "       [1.15023611, 0.        , 0.        , 0.        , 1.0582596 ,\n",
       "        0.37728426, 0.        , 0.        , 0.49094529, 1.40132904],\n",
       "       [1.20165866, 0.        , 0.        , 0.        , 0.36867938,\n",
       "        0.02011754, 0.        , 2.04187731, 0.35059333, 0.15891649],\n",
       "       [0.87188742, 0.        , 0.01976177, 0.        , 1.62011792,\n",
       "        0.6963296 , 0.04203325, 0.        , 1.20494663, 0.        ]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndl = t_update(ndl)\n",
    "from nodeleys.math.forward_math_func import *\n",
    "x = ndl.Node(cupy.random.normal(size=(10,10)))\n",
    "print(x.tensor)\n",
    "node_relu(x).tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0.get_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.sqrt(784.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "\n",
    "a = cupy.array([[\n",
    "  [1,2,3],\n",
    "  [4,5,6],\n",
    "  [7,8,9]\n",
    "],[\n",
    "  [7,8,9],\n",
    "  [10,11,12],\n",
    "  [0,1,2]\n",
    "]])\n",
    "\n",
    "cupy.max(a, axis=[-2,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = ndl.Node(cupy.random.normal(size=(1516,1516)), name='x')\n",
    "data_y = ndl.Node(cupy.random.normal(size=(1516,1)), name='y')\n",
    "data_y.is_constant = True\n",
    "\n",
    "w0 = ndl.Node(cupy.random.normal(size=(1516,784*2)), name='w0')\n",
    "b0 = ndl.Node(cupy.random.normal(size=(1,784*2)), name='b0')\n",
    "\n",
    "w1 = ndl.Node(cupy.random.normal(size=(784*2,64)), name='w1')\n",
    "b1 = ndl.Node(cupy.random.normal(size=(1,64)), name='b1')\n",
    "\n",
    "w1r = ndl.Node(cupy.random.normal(size=(784*2,64)), name='w1r')\n",
    "b1r = ndl.Node(cupy.random.normal(size=(1,64)), name='b1r')\n",
    "\n",
    "rR = ndl.Node(cupy.random.normal(size=(64,64)), name='rR')\n",
    "bR = ndl.Node(cupy.random.normal(size=(1,64)), name='bR')\n",
    "\n",
    "rL = ndl.Node(cupy.random.normal(size=(64,64)), name='rL')\n",
    "bL = ndl.Node(cupy.random.normal(size=(1,64)), name='bL')\n",
    "\n",
    "w2 = ndl.Node(cupy.random.normal(size=(64,1)), name='w2')\n",
    "b2 = ndl.Node(cupy.random.normal(size=(1,1)), name='b2')\n",
    "\n",
    "RANX = ndl.Node(cupy.random.normal(size=(1516, 64)), name='RANX')\n",
    "\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "  tf_img = tf.Variable(tf.random.normal(shape=(1516, 2, 28, 28), dtype=tf.float64))\n",
    "  tf_data_x = tf.Variable(tf.convert_to_tensor(data_x.tensor.get()))\n",
    "  tf_data_y = tf.Variable(tf.convert_to_tensor(data_y.tensor.get()))\n",
    "  tf_w0 = tf.Variable(tf.convert_to_tensor(w0.tensor.get()))\n",
    "  tf_b0 = tf.Variable(tf.convert_to_tensor(b0.tensor.get()))\n",
    "  tf_w1 = tf.Variable(tf.convert_to_tensor(w1.tensor.get()))\n",
    "  tf_b1 = tf.Variable(tf.convert_to_tensor(b1.tensor.get()))\n",
    "  tf_w1r = tf.Variable(tf.convert_to_tensor(w1r.tensor.get()))\n",
    "  tf_b1r = tf.Variable(tf.convert_to_tensor(b1r.tensor.get()))\n",
    "  tf_rR = tf.Variable(tf.convert_to_tensor(rR.tensor.get()))\n",
    "  tf_bR = tf.Variable(tf.convert_to_tensor(bR.tensor.get()))\n",
    "  tf_rL = tf.Variable(tf.convert_to_tensor(rL.tensor.get()))\n",
    "  tf_bL = tf.Variable(tf.convert_to_tensor(bL.tensor.get()))\n",
    "  tf_w2 = tf.Variable(tf.convert_to_tensor(w2.tensor.get()))\n",
    "  tf_b2 = tf.Variable(tf.convert_to_tensor(b2.tensor.get()))\n",
    "  tf_RANX = tf.Variable(tf.convert_to_tensor(RANX.tensor.get()))\n",
    "\n",
    "  tf_flatten = tf.reshape(tf_img, shape=(tf_img.shape[0], -1))\n",
    "  # tf_gamma0 = tf_data_x @ tf_w0\n",
    "  tf_y0 = tf_flatten + tf_b0\n",
    "  tf_gamma1 = tf_y0 @ tf_w1\n",
    "  tf_y1 = tf_gamma1 + tf_b1\n",
    "  tf_gamma1r = tf_y0 @ tf_w1r\n",
    "  tf_gamma1rs = tf_y0 @ tf_w1r\n",
    "  tf_y1rs = tf_gamma1r + tf_b1r\n",
    "  tf_y1r = tf.where(tf_y1rs <= 0, x=0., y=tf_RANX * tf_y1rs)\n",
    "\n",
    "\n",
    "  tf_gamma_rR = tf_y1r @ tf_rR\n",
    "  tf_y_rR = tf_gamma_rR + tf_bR\n",
    "\n",
    "  tf_gamma_rL = tf_y1r @ tf_rL\n",
    "  tf_y_rL = tf_gamma_rL + tf_bL\n",
    "\n",
    "  tf_y_rRL = tf_y_rR + tf_y_rL\n",
    "  tf_y_rRL = 5./(5. + tf_y_rRL)\n",
    "\n",
    "  tf_merge = tf_y1r + tf_y_rRL\n",
    "  tf_merge2 = tf_merge + tf_y1\n",
    "\n",
    "  tf_gamma2 = tf_merge2 @ tf_w2\n",
    "  tf_y2 = tf_gamma2 + tf_b2\n",
    "  tf_diff = tf_y2 - tf_data_y\n",
    "  tf_squared = tf_diff ** 2\n",
    "  tf_redsum = tf.reduce_sum(tf_squared, axis=0, keepdims=True)\n",
    "  tf_loss = tf_redsum / 16.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_flatten[0,900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_img[0,1,4,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape(g.gradient(tf_loss, tf_flatten), shape=tf_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.gradient(tf_loss, tf_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.gradient(tf_loss, tf_img)[0,:,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "ndl = t_update(ndl)\n",
    "from nodeleys.math.forward_math_func import *\n",
    "from nodeleys.graph import Switch\n",
    "\n",
    "data_x = ndl.Node(cupy.random.normal(size=(1516,1516)), name='x')\n",
    "img = ndl.Node(cupy.random.normal(size=(1516,2,28,28)), name='img')\n",
    "\n",
    "data_y = ndl.Node(cupy.random.normal(size=(1516,1)), name='y')\n",
    "data_y.is_constant = True\n",
    "\n",
    "w0 = ndl.Node(cupy.random.normal(size=(1516,784*2)), name='w0')\n",
    "b0 = ndl.Node(cupy.random.normal(size=(1,784*2)), name='b0')\n",
    "\n",
    "w1 = ndl.Node(cupy.random.normal(size=(784*2,64)), name='w1')\n",
    "b1 = ndl.Node(cupy.random.normal(size=(1,64)), name='b1')\n",
    "\n",
    "w1r = ndl.Node(cupy.random.normal(size=(784*2,64)), name='w1r')\n",
    "b1r = ndl.Node(cupy.random.normal(size=(1,64)), name='b1r')\n",
    "\n",
    "rR = ndl.Node(cupy.random.normal(size=(64,64)), name='rR')\n",
    "bR = ndl.Node(cupy.random.normal(size=(1,64)), name='bR')\n",
    "\n",
    "rL = ndl.Node(cupy.random.normal(size=(64,64)), name='rL')\n",
    "bL = ndl.Node(cupy.random.normal(size=(1,64)), name='bL')\n",
    "\n",
    "w2 = ndl.Node(cupy.random.normal(size=(64,1)), name='w2')\n",
    "b2 = ndl.Node(cupy.random.normal(size=(1,1)), name='b2')\n",
    "\n",
    "RANX = ndl.Node(cupy.random.normal(size=(1516, 64)), name='RANX')\n",
    "\n",
    "flatten = node_flatten(img, 'flatten')\n",
    "\n",
    "gamma0 = node_matmul(data_x, w0, 'gamma0')\n",
    "y0 = node_add(flatten, b0, 'y0')\n",
    "\n",
    "gamma1 = node_matmul(y0, w1, 'gamma1')\n",
    "y1 = node_add(gamma1, b1, 'y1')\n",
    "\n",
    "gamma1r = node_matmul(y0, w1r, 'gamma1r')\n",
    "y1rs = node_add(gamma1r, b1r, 'y1rs')\n",
    "\n",
    "y1r = Switch(y1rs, [gamma1r], [node_mul(y1rs, 0.), node_mul(RANX, y1rs), node_mul(y1rs, 0.)], ['x<=0', 'x>0', 'x<=0'], 'y1r').compile()\n",
    "# y1r = Virtual([y1r_pre], [node_mul(y1r_pre, 0.), y1r_pre], ['[0]<0', '[0]>=0'], 'y1r').compile()\n",
    "\n",
    "gamma_rR = node_matmul(y1r, rR, 'gamma_rR')\n",
    "y_rR = node_add(gamma_rR, bR, 'y_rR')\n",
    "\n",
    "gamma_rL = node_matmul(y1r, rL, 'gamma_rL')\n",
    "y_rL = node_add(gamma_rL, bL, 'y_rL')\n",
    "\n",
    "y_rRL = node_add(y_rR, y_rL, 'y_rRL')\n",
    "y_rRL = node_div(5., node_add(5., y_rRL))\n",
    "\n",
    "merge = node_add(y1r, y_rRL, 'merge')\n",
    "merge2 = node_add(merge, y1, 'merge2')\n",
    "\n",
    "gamma2 = node_matmul(merge2, w2, 'gamma2')\n",
    "y2 = node_add(gamma2, b2, 'y2')\n",
    "\n",
    "diff = node_sub(y2, data_y, 'diff')\n",
    "squared = node_pow(diff, 2., 'squared')\n",
    "redsum = node_redsum(squared, 0, 'redsum')\n",
    "loss = node_div(redsum, 16., 'loss')\n",
    "\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "  tf_img = tf.Variable(tf.convert_to_tensor(img.tensor.get()))\n",
    "  tf_data_x = tf.Variable(tf.convert_to_tensor(data_x.tensor.get()))\n",
    "  tf_data_y = tf.Variable(tf.convert_to_tensor(data_y.tensor.get()))\n",
    "  tf_w0 = tf.Variable(tf.convert_to_tensor(w0.tensor.get()))\n",
    "  tf_b0 = tf.Variable(tf.convert_to_tensor(b0.tensor.get()))\n",
    "  tf_w1 = tf.Variable(tf.convert_to_tensor(w1.tensor.get()))\n",
    "  tf_b1 = tf.Variable(tf.convert_to_tensor(b1.tensor.get()))\n",
    "  tf_w1r = tf.Variable(tf.convert_to_tensor(w1r.tensor.get()))\n",
    "  tf_b1r = tf.Variable(tf.convert_to_tensor(b1r.tensor.get()))\n",
    "  tf_rR = tf.Variable(tf.convert_to_tensor(rR.tensor.get()))\n",
    "  tf_bR = tf.Variable(tf.convert_to_tensor(bR.tensor.get()))\n",
    "  tf_rL = tf.Variable(tf.convert_to_tensor(rL.tensor.get()))\n",
    "  tf_bL = tf.Variable(tf.convert_to_tensor(bL.tensor.get()))\n",
    "  tf_w2 = tf.Variable(tf.convert_to_tensor(w2.tensor.get()))\n",
    "  tf_b2 = tf.Variable(tf.convert_to_tensor(b2.tensor.get()))\n",
    "  tf_RANX = tf.Variable(tf.convert_to_tensor(RANX.tensor.get()))\n",
    "\n",
    "  # tf_gamma0 = tf_data_x @ tf_w0\n",
    "  tf_flatten = tf.reshape(tf_img, shape=(tf_img.shape[0], -1))\n",
    "  tf_y0 = tf_flatten + tf_b0\n",
    "  tf_gamma1 = tf_y0 @ tf_w1\n",
    "  tf_y1 = tf_gamma1 + tf_b1\n",
    "  tf_gamma1r = tf_y0 @ tf_w1r\n",
    "  tf_gamma1rs = tf_y0 @ tf_w1r\n",
    "  print(id(tf_gamma1r))\n",
    "  print(id(tf_gamma1rs))\n",
    "  tf_y1rs = tf_gamma1r + tf_b1r\n",
    "  tf_y1r = tf.where(tf_y1rs <= 0, x=0., y=tf_RANX * tf_y1rs)\n",
    "\n",
    "\n",
    "  tf_gamma_rR = tf_y1r @ tf_rR\n",
    "  tf_y_rR = tf_gamma_rR + tf_bR\n",
    "\n",
    "  tf_gamma_rL = tf_y1r @ tf_rL\n",
    "  tf_y_rL = tf_gamma_rL + tf_bL\n",
    "\n",
    "  tf_y_rRL = tf_y_rR + tf_y_rL\n",
    "  tf_y_rRL = 5./(5. + tf_y_rRL)\n",
    "\n",
    "  tf_merge = tf_y1r + tf_y_rRL\n",
    "  tf_merge2 = tf_merge + tf_y1\n",
    "\n",
    "  tf_gamma2 = tf_merge2 @ tf_w2\n",
    "  tf_y2 = tf_gamma2 + tf_b2\n",
    "  tf_diff = tf_y2 - tf_data_y\n",
    "  tf_squared = tf_diff ** 2\n",
    "  tf_redsum = tf.reduce_sum(tf_squared, axis=0, keepdims=True)\n",
    "  tf_loss = tf_redsum / 16.\n",
    "  \n",
    "start_time = time.time()\n",
    "grads = g.gradient(tf_loss, [tf_w0, tf_w1, tf_w2, tf_b0, tf_gamma_rR])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "loss.adic.set_as_objective()\n",
    "loss.adic.begin_backprop()\n",
    "w0.get_gradient()\n",
    "w1.get_gradient()\n",
    "w2.get_gradient()\n",
    "b0.get_gradient()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.gradient(tf_loss, tf_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten.get_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration: Numpy for Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[0,  1,  2,  3,  4],\n",
    "              [ 5,  6,  7,  8,  9],\n",
    "              [10, 11, 12, 13, 14],\n",
    "              [15, 16, 17, 18, 19],\n",
    "              [20, 21, 22, 23, 24]], dtype=np.float64)\n",
    "\n",
    "sub_shape = (3,3)\n",
    "view_shape = tuple(np.subtract(a.shape, sub_shape) + 1) + sub_shape\n",
    "strides = a.strides + a.strides\n",
    "\n",
    "sub_matrices = np.lib.stride_tricks.as_strided(a,view_shape,strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.uniform(size=(20,3,6,6), high=100).astype(int)\n",
    "sub_shape = (20,3,3,3)\n",
    "view_shape = tuple(np.subtract(a.shape, sub_shape) + 1) + sub_shape\n",
    "strides = a.strides + a.strides\n",
    "sub_matrices = np.lib.stride_tricks.as_strided(a, view_shape[2:], strides[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_w = 3\n",
    "stride_w = 1\n",
    "\n",
    "w = 1 + int((a.shape[2] - kernel_w)/stride_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_matrices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_matrices[1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "new_width = 1 + int((domain_width - kernel_width)/stride_width)\n",
    "new_height = 1 + int((domain_height - kernel_height)/stride_height)\n",
    "\n",
    "pick_shape = (new_height, new_width, batch_size, channel, kernel_height, kernel_width)\n",
    "\n",
    "block_strides = cupy.array((block.strides + block.strides)[2:])\n",
    "block_stride_strides = cupy.array((block.strides[2]*(stride_height-1), block.strides[3]*(stride_width-1), 0, 0,0,0))\n",
    "\n",
    "memory_skip = (block_strides+block_stride_strides).get()\n",
    "\n",
    "sub_block = cupy.lib.stride_tricks.as_strided(block, pick_shape, memory_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(block[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sub_block[1,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cupy import ndarray\n",
    "from typing import List, Any, Tuple, Set\n",
    "\n",
    "import cupy\n",
    "import nodeleys as ndl\n",
    "import sys\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "\n",
    "def t_update(ndl):\n",
    "  pending_for_deletion = []\n",
    "  for i in sys.modules:\n",
    "    if i.split('.')[0] == 'nodeleys':\n",
    "      pending_for_deletion.append(i)\n",
    "\n",
    "  # print(pending_for_deletion)\n",
    "    \n",
    "  for i in pending_for_deletion:\n",
    "    del sys.modules[i]\n",
    "\n",
    "  import nodeleys as ndl\n",
    "  return ndl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "channel = 3\n",
    "\n",
    "kernel_width = 3\n",
    "kernel_height = 3\n",
    "\n",
    "domain_width = 28\n",
    "domain_height = 28\n",
    "\n",
    "stride_width = 2\n",
    "stride_height = 2\n",
    "\n",
    "block = cupy.random.uniform(size=(batch_size, channel, domain_width, domain_height), high=100).astype(int)\n",
    "kernel = cupy.random.uniform(size=(5, channel, kernel_width, kernel_height), high = 10).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndl = t_update(ndl)\n",
    "from nodeleys.math.forward_math_func import *\n",
    "from nodeleys.graph import Switch\n",
    "\n",
    "blocks = ndl.Node(block, name='blocks')\n",
    "kernels = ndl.Node(kernel, name='kernels')\n",
    "\n",
    "conv2d = node_conv2d(blocks, kernels, (1,1), 'conv2d')\n",
    "maxpool2d = node_maxpool2d(conv2d, pool_size=(3,2), strides=(2,2), name='maxpool2d')\n",
    "flatten = node_flatten(maxpool2d, 'flatten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def tfct(node):\n",
    "  return tf.convert_to_tensor(node.tensor.get(), dtype=tf.float64)\n",
    "\n",
    "ndl = t_update(ndl)\n",
    "from nodeleys.math.forward_math_func import *\n",
    "from nodeleys.graph import Switch\n",
    "\n",
    "# dataset\n",
    "img = ndl.Node(cupy.random.normal(scale=3.0, size=(166, 3, 28, 28)), name='img')\n",
    "tf_img = tfct(img)\n",
    "\n",
    "data_y = ndl.Node(cupy.random.normal(scale=10., size=(166, 1)), name='data_y')\n",
    "tf_data_y = tfct(data_y)\n",
    "\n",
    "# config\n",
    "kernels = ndl.Node(cupy.random.normal(size=(2, 3, 3, 3)), name= 'kernel', is_trainable=True)\n",
    "tf_kernels = tfct(kernels)\n",
    "W1 = ndl.Node(cupy.random.normal(size=(1352, 500)), name='W1', is_trainable=True)\n",
    "tf_W1 = tfct(W1)\n",
    "W2 = ndl.Node(cupy.random.normal(size=(500, 32)), name='W2', is_trainable=True)\n",
    "tf_W2 = tfct(W2)\n",
    "W3 = ndl.Node(cupy.random.normal(size=(32, 1)), name='W3', is_trainable=True)\n",
    "tf_W3 = tfct(W3)\n",
    "\n",
    "# conv2d\n",
    "conv2d = node_conv2d(img, kernels, (1,1), name='conv2d')\n",
    "\n",
    "# flatten\n",
    "flatten = node_flatten(conv2d, name='flatten')\n",
    "\n",
    "# dense\n",
    "dense1 = node_matmul(flatten, W1, name='dense1') # 1352 -> 500\n",
    "dense2 = node_matmul(dense1, W2, name='dense2') # 500 -> 32\n",
    "dense3 = node_matmul(dense2, W3, name='dense3') # 32 -> 1\n",
    "\n",
    "delta = node_sub(dense3, data_y, name='delta')\n",
    "L = node_redsum(delta, axis=0, name='redsum')\n",
    "tf_L = tfct(L)\n",
    "\n",
    "L.adic.set_as_objective()\n",
    "start = time.time()\n",
    "_, _ = L.adic.begin_backprop()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as g:\n",
    "  tf_kernels = tf.Variable(tf_kernels)\n",
    "  tf_img = tf.Variable(tf_img)\n",
    "  tf_conv2d = tf.nn.conv2d(tf_img, tf.transpose(tf_kernels, perm=[2,3,1,0]), strides=[1,1], padding='VALID', data_format='NCHW')\n",
    "  tf_flatten = tf.keras.layers.Flatten(dtype=tf.float64)(tf_conv2d)\n",
    "  tf_dense1 = tf_flatten @ tf.Variable(tf_W1)\n",
    "  tf_dense2 = tf_dense1 @ tf.Variable(tf_W2)\n",
    "  tf_dense3 = tf_dense2 @ tf.Variable(tf_W3)\n",
    "  tf_delta = tf_dense3 - tf.Variable(tf_data_y)\n",
    "  tf_L = tf.math.reduce_sum(input_tensor=tf_delta, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g.gradient(tf_L, tf_img)\n",
    "img.get_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g.gradient(tf_L, [tf_img])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cupy.array([1,2,3]).get()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def consecutive_overlap_sum(arrays):\n",
    "    # Stack all arrays into a 2D array\n",
    "    stacked = np.vstack(arrays)\n",
    "    \n",
    "    # Create a triangular mask to handle shifts\n",
    "    length = stacked.shape[1]\n",
    "    tri_mask = np.tri(stacked.shape[0], length, dtype=int)\n",
    "    \n",
    "    # Element-wise multiply the mask with the stacked array\n",
    "    shifted_stacked = stacked * tri_mask[:, :length]\n",
    "    \n",
    "    # Sum along the columns to get the final result\n",
    "    result = shifted_stacked.sum(axis=0)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example with 100 arrays of length 4\n",
    "arrays = [np.random.randint(1, 10, 4) for _ in range(100)]\n",
    "print(arrays)\n",
    "result = consecutive_overlap_sum(arrays)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256\n",
    "H = 128\n",
    "W = 128\n",
    "a = np.random.uniform(high=100, size=(N,28,H,W)).astype(int)\n",
    "\n",
    "start = time.time()\n",
    "result = np.zeros(shape=(N+3,28,H,W))\n",
    "for i in range(N): \n",
    "  result[i+3,:,:,:] += a[i]\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "\n",
    "# print(a[0],'\\n')\n",
    "# print(a[1],'\\n')\n",
    "# print(a[2],'\\n')\n",
    "# print(a[3],'\\n')\n",
    "\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate random 3x3 matrices\n",
    "a = np.random.uniform(high=100, size=(3, 3)).astype(int)\n",
    "b = np.random.uniform(high=100, size=(3, 3)).astype(int)\n",
    "c = np.random.uniform(high=100, size=(3, 3)).astype(int)\n",
    "\n",
    "def vectorized_consecutive_pad_sum(*matrices):\n",
    "    # Number of matrices and their shape\n",
    "    num_matrices = len(matrices)\n",
    "    n, m = matrices[0].shape\n",
    "\n",
    "    # Create a large matrix (3D) for all padded matrices\n",
    "    max_padding = num_matrices - 1\n",
    "    padded_matrices = np.zeros((num_matrices, n, m + max_padding))\n",
    "\n",
    "    # Use slicing to place each matrix in the correct padded position\n",
    "    matrix_array = np.array(matrices)\n",
    "    row_index = np.arange(n)  # All rows\n",
    "    col_index = np.arange(m)  # All columns\n",
    "\n",
    "    # Broadcast each matrix into the larger array with the appropriate padding\n",
    "    padded_matrices[:, row_index, col_index + np.arange(num_matrices)] = matrix_array\n",
    "\n",
    "    # Sum along the first axis (over all matrices)\n",
    "    result = np.sum(padded_matrices, axis=0)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Call the function\n",
    "result = vectorized_consecutive_pad_sum(a, b, c)\n",
    "\n",
    "print(\"Matrix A:\\n\", a, \"\\n\")\n",
    "print(\"Matrix B:\\n\", b, \"\\n\")\n",
    "print(\"Matrix C:\\n\", c, \"\\n\")\n",
    "print(\"Resulting Sum:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfct(node):\n",
    "  return tf.convert_to_tensor(node.tensor.get(), dtype=tf.float64)\n",
    "\n",
    "ndl = t_update(ndl)\n",
    "from nodeleys.math.forward_math_func import *\n",
    "from nodeleys.graph import Switch\n",
    "from nodeleys.model import NodeleysModel\n",
    "\n",
    "# dataset\n",
    "img = ndl.Node(cupy.random.normal(size=(20, 3, 28, 28)), name='img')\n",
    "data_y = ndl.Node(cupy.random.normal(size=(20, 1)), name='data_y')\n",
    "\n",
    "class Model(NodeleysModel):\n",
    "  def __init__(self):\n",
    "    self.kernels = ndl.Node(cupy.random.normal(size=(2, 3, 3, 3)), name= 'kernel', is_trainable=True)\n",
    "    self.W1 = ndl.Node(cupy.random.normal(size=(1352, 500)), name='W1', is_trainable=True)\n",
    "    self.W2 = ndl.Node(cupy.random.normal(size=(500, 32)), name='W2', is_trainable=True)\n",
    "    self.W3 = ndl.Node(cupy.random.normal(size=(32, 1)), name='W3', is_trainable=True)\n",
    "\n",
    "  def graph(self):\n",
    "    conv2d = node_conv2d(img, self.kernels, (1,1), name='conv2d')\n",
    "    flatten = node_flatten(conv2d, name='flatten')\n",
    "\n",
    "    dense1 = node_matmul(flatten, self.W1, name='dense1') # 1352 -> 500\n",
    "    dense2 = node_matmul(dense1, self.W2, name='dense2') # 500 -> 32\n",
    "    dense3 = node_matmul(dense2, self.W3, name='dense3') # 32 -> 1\n",
    "\n",
    "    delta = node_sub(dense3, data_y, name='delta')\n",
    "    L = node_redsum(delta, axis=0, name='redsum')\n",
    "    tf_L = tfct(L)\n",
    "\n",
    "    L.adic.set_as_objective()\n",
    "    L.adic.begin_backprop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.W1.get_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "  def __init__(self, units):\n",
    "    self.units = units\n",
    "  \n",
    "  def __call__(self, prev):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfct(node):\n",
    "  return tf.convert_to_tensor(node.tensor.get(), dtype=tf.float64)\n",
    "\n",
    "ndl = t_update(ndl)\n",
    "from nodeleys.math.forward_math_func import *\n",
    "from nodeleys.graph import Switch\n",
    "\n",
    "# dataset\n",
    "img = ndl.Node(cupy.random.normal(size=(20, 3, 28, 28)), name='img')\n",
    "data_y = ndl.Node(cupy.random.normal(size=(20, 1)), name='data_y')\n",
    "\n",
    "# config\n",
    "kernels = ndl.Node(cupy.random.normal(size=(2, 3, 3, 3)), name= 'kernel', is_trainable=True)\n",
    "W1 = ndl.Node(cupy.random.normal(size=(1352, 500)), name='W1', is_trainable=True)\n",
    "W2 = ndl.Node(cupy.random.normal(size=(500, 32)), name='W2', is_trainable=True)\n",
    "W3 = ndl.Node(cupy.random.normal(size=(32, 1)), name='W3', is_trainable=True)\n",
    "\n",
    "\n",
    "# conv2d\n",
    "conv2d = node_conv2d(img, kernels, (1,1), name='conv2d')\n",
    "\n",
    "# flatten\n",
    "flatten = node_flatten(conv2d, name='flatten')\n",
    "\n",
    "# dense\n",
    "dense1 = node_matmul(flatten, W1, name='dense1') # 1352 -> 500\n",
    "dense2 = node_matmul(dense1, W2, name='dense2') # 500 -> 32\n",
    "dense3 = node_matmul(dense2, W3, name='dense3') # 32 -> 1\n",
    "\n",
    "delta = node_sub(dense3, data_y, name='delta')\n",
    "L = node_redsum(delta, axis=0, name='redsum')\n",
    "\n",
    "L.adic.set_as_objective()\n",
    "L.adic.begin_backprop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G:\n",
    "  def auxer(self, mex):\n",
    "    def inner():\n",
    "      print(type(mex))\n",
    "      print('hex')\n",
    "      mex()\n",
    "    return inner\n",
    "  \n",
    "g = G()\n",
    "\n",
    "@g.auxer\n",
    "def aa():\n",
    "  print('hast')\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
